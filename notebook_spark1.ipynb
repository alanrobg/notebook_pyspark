{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b992ac1-b410-4afb-8a60-abf57532b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encuentra a Spark en la PC\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac7f549-601e-4b6b-8ff7-9dd9f97440d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importa la sesion\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d69a40f-6178-4de1-be2c-2ec52c523df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura la sesiÃ³n de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PruebaPySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eeb8a9e-ee47-48f8-9968-090c1fac0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un DataFrame de ejemplo\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "columns = [\"Nombre\", \"Edad\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7fa8d0-20d8-4c4a-adee-984cc7747ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "| Nombre|Edad|\n",
      "+-------+----+\n",
      "|  Alice|   1|\n",
      "|    Bob|   2|\n",
      "|Charlie|   3|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f75713b6-e63b-4050-9815-29278fa748b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "| Nombre|Edad|Edad_doble|\n",
      "+-------+----+----------+\n",
      "|  Alice|   1|         2|\n",
      "|    Bob|   2|         4|\n",
      "|Charlie|   3|         6|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Edad_doble\", df[\"Edad\"] * 2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8dc9f5c-f70e-4d76-beb2-6d3f534c8536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+\n",
      "| Nombre|Edad|Edad_doble|edad_triple|\n",
      "+-------+----+----------+-----------+\n",
      "|  Alice|   1|         2|          3|\n",
      "|    Bob|   2|         4|          6|\n",
      "|Charlie|   3|         6|          9|\n",
      "+-------+----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df=df.withColumn(\"edad_triple\",col(\"edad\")*3)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bcedde1-0932-4839-8794-20e8be438221",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10e6aec5-27bb-4d35-b442-1bbf91b6b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Consumo_API\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49133cea-eda2-430c-8137-067ff6b33e07",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperationException",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 30\u001b[0m\n\u001b[0;32m      5\u001b[0m mySchema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m      6\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m,IntegerType(),\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      7\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(),\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     ]),\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m ])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#df = spark.read.schema(mySchema).json(url_api)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:425\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[1;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator: Iterable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable:\n",
      "File \u001b[1;32m~\\Documents\\spark-3.5.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Documents\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mUnsupportedOperationException\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "url_api='https://jsonplaceholder.typicode.com/users'\n",
    "\n",
    "mySchema = StructType([\n",
    "    StructField(\"id\",IntegerType(),True),\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"username\",StringType(),True),\n",
    "    StructField(\"email\",StringType(),True),\n",
    "    StructField(\"address\",StructType([\n",
    "        StructField(\"street\",StringType(),True),\n",
    "        StructField(\"suit\",StringType(),True),\n",
    "        StructField(\"city\",StringType(),True),\n",
    "        StructField(\"zipcode\",StringType(),True),\n",
    "        StructField(\"geo\",StructType([\n",
    "                StructField(\"lat\",IntegerType(),True),\n",
    "                StructField(\"lng\",IntegerType(),True),\n",
    "        ]),True)\n",
    "    ]),True),\n",
    "    StructField(\"phone\",StringType(),True),\n",
    "    StructField(\"web\",StringType(),True),\n",
    "    StructField(\"company\",StructType([\n",
    "        StructField(\"name\",StringType(),True),\n",
    "        StructField(\"phrase\",StringType(),True),\n",
    "        StructField(\"bs\",StringType(),True)\n",
    "    ]),True)\n",
    "])\n",
    "\n",
    "#df = spark.read.schema(mySchema).json(url_api)\n",
    "df = spark.read.json(url_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d976bf8-7023-4729-a945-f31bbea120ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------+-------------+--------------+--------------------+------------------+\n",
      "|CODMES|           CODOPECTA|CODCLAVEOPECTA|CODCTATARJETA|      PRODUCTO|SEGMENTO_REGULATORIO|MTODEUDACASTIGOSOL|\n",
      "+------+--------------------+--------------+-------------+--------------+--------------------+------------------+\n",
      "|202201|305K0000000007507602|     177392057|         NULL|BOLSON CONSUMO|             Consumo|             720.0|\n",
      "|202201|305K0000000007507603|     177392058|         NULL|BOLSON CONSUMO|             Consumo|            2134.0|\n",
      "|202201|305K0000000007507604|     177392059|         NULL|BOLSON CONSUMO|             Consumo|             500.0|\n",
      "|202201|310K0000000007507609|     177392068|         NULL|BOLSON CONSUMO|             Consumo|           87883.0|\n",
      "|202201|310K0000000007507611|     177392070|         NULL|BOLSON CONSUMO|             Consumo|           23647.0|\n",
      "|202201|310K0000000007507612|     177392071|         NULL|BOLSON CONSUMO|             Consumo|             450.0|\n",
      "|202201|310K0000000007507619|     177392078|         NULL|BOLSON CONSUMO|             Consumo|             580.0|\n",
      "|202201|310K0000000007507621|     177392080|         NULL|BOLSON CONSUMO|             Consumo|             240.0|\n",
      "|202201|191K0000000007492489|     177210611|         NULL|       CONSUMO|             Consumo|           56001.0|\n",
      "|202201|191K0000000007493885|     177211647|         NULL|       CONSUMO|             Consumo|             750.0|\n",
      "|202201|194K0000000007527343|     177882043|         NULL|BOLSON CONSUMO|             Consumo|           48271.0|\n",
      "|202201|194K0000000007527345|     177882045|         NULL|BOLSON CONSUMO|             Consumo|            1000.0|\n",
      "|202201|194K0000000007527346|     177882046|         NULL|BOLSON CONSUMO|             Consumo|            1460.0|\n",
      "|202201|215K0000000007522752|     177804519|         NULL|BOLSON CONSUMO|             Consumo|            1000.0|\n",
      "|202201|215K0000000007522759|     177804526|         NULL|BOLSON CONSUMO|             Consumo|           55219.0|\n",
      "|202201|235K0000000007522764|     177804533|         NULL|BOLSON CONSUMO|             Consumo|             288.0|\n",
      "|202201|255K0000000007522767|     177804547|         NULL|BOLSON CONSUMO|             Consumo|             500.0|\n",
      "|202201|280K0000000007522773|     177804558|         NULL|BOLSON CONSUMO|             Consumo|             450.0|\n",
      "|202201|192K0000000007525146|     177840505|         NULL|BOLSON CONSUMO|             Consumo|           91885.0|\n",
      "|202201|192K0000000007525148|     177840507|         NULL|BOLSON CONSUMO|             Consumo|           32038.0|\n",
      "+------+--------------------+--------------+-------------+--------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"castigos\").getOrCreate()\n",
    "\n",
    "ruta = 'C:\\\\Users\\\\Raul\\\\Documents\\\\castigos.csv'\n",
    "\n",
    "df = spark.read.csv(ruta,sep=\";\", header=True, inferSchema=True)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f181b7a4-0e3f-416f-a57e-019c8f1433d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de registros mayores a 202207 es 91029\n"
     ]
    }
   ],
   "source": [
    "df1=df.where(\"codmes>202207\")\n",
    "ctd_reg=df1.count()\n",
    "\n",
    "print(f\"La cantidad de registros mayores a 202207 es {ctd_reg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3b320f4-cf18-4328-beee-cbad040371a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|segmento_regulatorio|\n",
      "+--------------------+\n",
      "|         Comerciales|\n",
      "|Hipotecario para ...|\n",
      "|             Consumo|\n",
      "|                 MES|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_prod=df.select(\"segmento_regulatorio\").distinct()\n",
    "df_prod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8ab92f4-1044-4af4-bf77-a8f1b0778d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_consumo = df.filter((col('codmes')=='202201') & (col('SEGMENTO_REGULATORIO')=='Consumo'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85e3f683-f75f-4ba2-a741-d01abaa4408f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8157\n"
     ]
    }
   ],
   "source": [
    "print(df_consumo.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
